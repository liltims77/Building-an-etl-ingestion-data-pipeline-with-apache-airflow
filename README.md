# Building-and-ingesting-data-through-pipeline
This project involves building  pipeline for a data set transforming data into scripts, storing and querying in postgres database (Pgcli and Pgadmin). Ingested data is then dockerized and Apache Airflow is used to monitor data workflow in AWS cloud storage.

# Project Description
This project involves building pipeline for a data sets. The data sets (Ny_taxi) was gotten from[ny taxi page] (https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). A python pipeline was first built and jupyter notebook was used to insert data into postgres database (pgcli) . Another database was created (pgadmin) and merged with pgcli to run and query the data set more efficently and effectively using postgresql. The data set was then changed to parquet ingested and dockerized (ingesting data into docker container). Apache airflow was used to schedule the workflow of the dataset from download, change to parquet, injesting inti AWS S3 bucket and running the airflow directly on Amanzon Managed Workflow for Apache airflow (MWAA) services.
